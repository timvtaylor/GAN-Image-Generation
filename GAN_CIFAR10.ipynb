{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7fa39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "from matplotlib import pyplot\n",
    "\n",
    "(x_train, x_y), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "pyplot.imshow(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import vstack\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.datasets.mnist import load_data\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# discriminator architecture\n",
    "def discriminator_model(in_shape=(32,32,3)):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(64, 3, padding=\"same\", input_shape=in_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Conv2D(128, 3, strides=(2,2), padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Conv2D(128, 3, strides=(2,2), padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Conv2D(256, 3, strides=(2,2), padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(learning_rate=0.0002), metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# generator architecture\n",
    "def generator_model(latent_dim):\n",
    "    model = Sequential()\n",
    "    \n",
    "    n_nodes = 256 *4 * 4\n",
    "    model.add(Dense(n_nodes, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((4, 4, 256)))\n",
    "\n",
    "    model.add(Conv2DTranspose(128, 4, strides=(2,2), padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Conv2DTranspose(128, 4, strides=(2,2), padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Conv2DTranspose(128, 4, strides=(2,2), padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Conv2D(3, (3,3), activation=\"tanh\", padding=\"same\"))\n",
    "    return model\n",
    "\n",
    "# dc_gan architecture\n",
    "def define_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(generator)\n",
    "\n",
    "    model.add(discriminator)\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(learning_rate=0.0002))\n",
    "    return model\n",
    "\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    index = randint(0, dataset.shape[0], n_samples)\n",
    "    X = dataset[index]\n",
    "    y = np.ones((n_samples, 1))\n",
    "    return X, y\n",
    "\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    x_input = np.random.randn(latent_dim * n_samples)\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    "\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    X = generator.predict(x_input)\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    return X, y\n",
    "\n",
    "def save_plot(examples, epoch, n=7):\n",
    "    examples = (examples + 1) / 2.0\n",
    "    for i in range(n*n):\n",
    "        pyplot.subplot(n, n, 1 + i)\n",
    "        pyplot.axis(\"off\")\n",
    "        pyplot.imshow(examples[i])\n",
    "    filename = \"CIFAR10_plot_LatentDim25_e%03d.png\" % (epoch+1)\n",
    "    pyplot.savefig(filename)\n",
    "    pyplot.close()\n",
    "\n",
    "def save_model(epoch, generator, discriminator, dataset, latent_dim, n_samples=150):\n",
    "    X_real, y_real = generate_real_samples(dataset, n_samples)\n",
    "    x_fake, y_fake = generate_fake_samples(generator, latent_dim, n_samples)\n",
    "    save_plot(x_fake, epoch)\n",
    "    filename = \"CIFAR10_model_LatentDim25_%03d.h5\" % (epoch+1)\n",
    "    generator.save(filename)\n",
    "\n",
    "def train(generator, discriminator, gan_model, dataset, latent_dim, n_epochs=10, n_batch=128):\n",
    "    bat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "    half_batch = int(n_batch / 2)\n",
    "    for i in range(n_epochs):\n",
    "        print(i)\n",
    "        for j in range(bat_per_epo):\n",
    "            X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "            d_loss1, _ = discriminator.train_on_batch(X_real, y_real)\n",
    "            X_fake, y_fake = generate_fake_samples(generator, latent_dim, half_batch)\n",
    "            d_loss2, _ = discriminator.train_on_batch(X_fake, y_fake)\n",
    "            X_gan = generate_latent_points(latent_dim, n_batch)\n",
    "            y_gan = np.ones((n_batch, 1))\n",
    "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "            print(\">%d, %d/%d\" %\n",
    "                (i+1, j+1, bat_per_epo))\n",
    "        if (i+1) % 10 == 0:\n",
    "            save_model(i, generator, discriminator, dataset, latent_dim)\n",
    "            \n",
    "latent_dim = 50\n",
    "\n",
    "discriminator = discriminator_model()\n",
    "\n",
    "generator = generator_model(latent_dim)\n",
    "\n",
    "dc_gan = define_gan(generator, discriminator)\n",
    "\n",
    "cifar10_data = ((x_train - 127.5)/127.5).astype(\"float32\")\n",
    "\n",
    "train(generator, discriminator, dc_gan, cifar10_data, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2018f3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from matplotlib import pyplot\n",
    "\n",
    "def create_plot(examples, n):\n",
    "    for i in range(n * n):\n",
    "        pyplot.subplot(n, n, 1 + i)\n",
    "        pyplot.axis(\"off\")\n",
    "        pyplot.imshow(examples[i, :, :])\n",
    "    pyplot.show()\n",
    "\n",
    "model = load_model(\"CIFAR10_model_010.h5\")\n",
    "latent_points = generate_latent_points(100, 100)\n",
    "X = model.predict(latent_points)\n",
    "X = (X + 1) / 2.0\n",
    "#create_plot(X, 10)\n",
    "\n",
    "pyplot.imshow(X[14])\n",
    "filename = \"CIFAR10_kernel3.png\"\n",
    "pyplot.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1890414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.datasets import mnist\n",
    "import skimage as skim\n",
    "import skimage.transform\n",
    "\n",
    "def inception(dataset, epsilon):\n",
    "    inception_model = InceptionV3()\n",
    "    \n",
    "    inception_scores = []\n",
    "    for i in range(10):\n",
    "        batch = dataset[i*math.floor(dataset.shape[0]/10):(i+1)*math.floor(dataset.shape[0]/10)]\n",
    "        batch = batch.astype(\"float32\")\n",
    "        \n",
    "        # resize the images for the inception_model\n",
    "        copies = []\n",
    "        for image in batch:\n",
    "                copies.append(skim.transform.resize(image, (299,299,3)))\n",
    "        batch = copies\n",
    "        \n",
    "        batch = preprocess_input(np.asarray(batch))\n",
    "        \n",
    "        prob_yx = inception_model.predict(batch)\n",
    "        prob_y = expand_dims(prob_yx.mean(axis=0), 0)\n",
    "\n",
    "        first_calc = prob_yx * (np.log(prob_yx + epsilon) - np.log(prob_y + epsilon))\n",
    "        \n",
    "        inception_score = np.exp(np.mean(first_calc.sum(axis=1)))\n",
    "        \n",
    "        inception_scores.append(inception_score)\n",
    "\n",
    "    return np.mean(inception_scores)\n",
    "\n",
    "(real_images, _), (_, _) = mnist.load_data()\n",
    "score = inception(real_images[0:100], 1E-16)\n",
    "generated_score = inception(X, 1E-16)\n",
    "noisy = np.random.rand(32,32,3)\n",
    "noisy_score = inception(noisy, 1E-16)\n",
    "print(\"Generator stride of 2:\")\n",
    "print(\"Real MNIST score: \", score)\n",
    "print(\"Generated MNIST score: \", generated_score)\n",
    "print(\"Random noise score: \", noisy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659c7eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7addfaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
